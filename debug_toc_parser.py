#!/usr/bin/env python3
"""
ç›®å½•è§£æå™¨è°ƒè¯•å·¥å…·
å¸®åŠ©è¯Šæ–­ç›®å½•è§£æé—®é¢˜
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
from app.core.config import settings
from app.core.source import Source
from app.parsers.toc_parser import TocParser

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def debug_toc_parser():
    """è°ƒè¯•ç›®å½•è§£æå™¨"""
    print("ğŸ” ç›®å½•è§£æå™¨è°ƒè¯•å·¥å…·")
    print("=" * 60)
    
    # æµ‹è¯•URLå’Œä¹¦æº
    test_url = "https://www.0xs.net/txt/1.html"
    source_id = 11  # é›¶ç‚¹å°è¯´
    
    # åˆ›å»ºä¹¦æºå¯¹è±¡
    source_data = {
        "id": source_id,
        "name": "é›¶ç‚¹å°è¯´",
        "url": "https://www.0xs.net/",
        "enabled": True,
        "type": "html",
        "language": "zh_CN",
        "toc": {
            "list": ".catalog li a",
            "title": "text",
            "url": "href",
            "has_pages": False,
            "next_page": "",
            "baseUri": "https://www.0xs.net/",
            "timeout": 15
        }
    }
    
    source = Source(**source_data)
    parser = TocParser(source)
    
    print(f"æµ‹è¯•URL: {test_url}")
    print(f"ä¹¦æº: {source.name}")
    print(f"ç›®å½•è§„åˆ™: {source.rule.get('toc', {})}")
    
    # 1. æµ‹è¯•è·å–ç›®å½•URL
    print("\n1. æµ‹è¯•è·å–ç›®å½•URL...")
    toc_url = parser._get_toc_url(test_url)
    print(f"   ç›®å½•URL: {toc_url}")
    
    # 2. æµ‹è¯•è·å–HTML
    print("\n2. æµ‹è¯•è·å–HTML...")
    html = await parser._fetch_html(toc_url)
    if html:
        print(f"   HTMLé•¿åº¦: {len(html)}")
        print(f"   HTMLé¢„è§ˆ: {html[:200]}...")
    else:
        print("   âŒ è·å–HTMLå¤±è´¥")
        return
    
    # 3. æµ‹è¯•è§£æç›®å½•
    print("\n3. æµ‹è¯•è§£æç›®å½•...")
    soup = BeautifulSoup(html, "html.parser")
    
    # æµ‹è¯•ä¸åŒçš„é€‰æ‹©å™¨
    selectors = [
        ".catalog li a",
        ".catalog > div > ul > ul > li > a",
        ".catalog a",
        "li a",
        "a"
    ]
    
    for selector in selectors:
        elements = soup.select(selector)
        print(f"   é€‰æ‹©å™¨ '{selector}': æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
        if elements:
            print(f"   ç¬¬ä¸€ä¸ªå…ƒç´ : {elements[0]}")
            print(f"   ç¬¬ä¸€ä¸ªå…ƒç´ æ–‡æœ¬: {elements[0].get_text(strip=True)}")
            print(f"   ç¬¬ä¸€ä¸ªå…ƒç´ href: {elements[0].get('href', 'æ— href')}")
    
    # 4. æµ‹è¯•å®Œæ•´è§£æ
    print("\n4. æµ‹è¯•å®Œæ•´è§£æ...")
    chapters = await parser.parse(test_url)
    print(f"   è§£æç»“æœ: {len(chapters)} ä¸ªç« èŠ‚")
    
    if chapters:
        print("   ç« èŠ‚é¢„è§ˆ:")
        for i, chapter in enumerate(chapters[:5]):
            print(f"     {i+1}. {chapter.title} - {chapter.url}")
    else:
        print("   âŒ æ²¡æœ‰è§£æåˆ°ç« èŠ‚")
        
        # 5. è¯¦ç»†è°ƒè¯•
        print("\n5. è¯¦ç»†è°ƒè¯•...")
        chapters = parser._parse_toc(html, toc_url)
        print(f"   ç›´æ¥è§£æç»“æœ: {len(chapters)} ä¸ªç« èŠ‚")

async def test_alternative_selectors():
    """æµ‹è¯•æ›¿ä»£é€‰æ‹©å™¨"""
    print("\nğŸ”§ æµ‹è¯•æ›¿ä»£é€‰æ‹©å™¨")
    print("=" * 60)
    
    test_url = "https://www.0xs.net/txt/1.html"
    
    # åˆ›å»ºaiohttpä¼šè¯
    connector = aiohttp.TCPConnector(
        limit=5,
        ssl=False,
        use_dns_cache=True,
        ttl_dns_cache=300,
    )
    
    timeout = aiohttp.ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(
        connector=connector,
        timeout=timeout,
        headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
    ) as session:
        try:
            async with session.get(test_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, "html.parser")
                    
                    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ç›®å½•å…ƒç´ 
                    print("æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ç›®å½•å…ƒç´ ...")
                    
                    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    all_links = soup.find_all("a")
                    print(f"æ€»é“¾æ¥æ•°: {len(all_links)}")
                    
                    # æŸ¥æ‰¾åŒ…å«ç« èŠ‚ç›¸å…³æ–‡æœ¬çš„é“¾æ¥
                    chapter_links = []
                    for link in all_links:
                        text = link.get_text(strip=True)
                        href = link.get("href", "")
                        if any(keyword in text for keyword in ["ç¬¬", "ç« ", "èŠ‚", "å›"]):
                            chapter_links.append((text, href))
                    
                    print(f"å¯èƒ½çš„ç« èŠ‚é“¾æ¥: {len(chapter_links)}")
                    for i, (text, href) in enumerate(chapter_links[:10]):
                        print(f"  {i+1}. {text} - {href}")
                    
                    # æŸ¥æ‰¾catalogç›¸å…³å…ƒç´ 
                    catalog_elements = soup.find_all(class_=lambda x: x and "catalog" in x)
                    print(f"catalogç›¸å…³å…ƒç´ : {len(catalog_elements)}")
                    
                    # æŸ¥æ‰¾listç›¸å…³å…ƒç´ 
                    list_elements = soup.find_all(class_=lambda x: x and "list" in x)
                    print(f"listç›¸å…³å…ƒç´ : {len(list_elements)}")
                    
                else:
                    print(f"è¯·æ±‚å¤±è´¥: {response.status}")
                    
        except Exception as e:
            print(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    # è¿è¡Œè°ƒè¯•
    asyncio.run(debug_toc_parser())
    asyncio.run(test_alternative_selectors())