#!/usr/bin/env python3
"""
ç®€åŒ–çš„ç›®å½•è§£æè°ƒè¯•å·¥å…·
ç›´æ¥æµ‹è¯•æ ¸å¿ƒé€»è¾‘
"""

import requests
import json
from bs4 import BeautifulSoup

def simple_toc_debug():
    """ç®€åŒ–çš„ç›®å½•è§£æè°ƒè¯•"""
    print("ğŸ” ç®€åŒ–çš„ç›®å½•è§£æè°ƒè¯•")
    print("=" * 50)
    
    # æµ‹è¯•URL
    test_url = "https://www.0xs.net/txt/1.html"
    
    print(f"æµ‹è¯•URL: {test_url}")
    
    # 1. ç›´æ¥è·å–HTML
    print("\n1. ç›´æ¥è·å–HTML...")
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://www.0xs.net/',
        }
        
        response = requests.get(test_url, headers=headers, timeout=30, verify=False)
        print(f"   å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code == 200:
            html = response.text
            print(f"   HTMLé•¿åº¦: {len(html)}")
            print(f"   HTMLé¢„è§ˆ: {html[:200]}...")
            
            # 2. è§£æHTML
            print("\n2. è§£æHTML...")
            soup = BeautifulSoup(html, "html.parser")
            
            # 3. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            print("\n3. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥...")
            all_links = soup.find_all("a")
            print(f"   æ€»é“¾æ¥æ•°: {len(all_links)}")
            
            # 4. æŸ¥æ‰¾ç« èŠ‚ç›¸å…³é“¾æ¥
            print("\n4. æŸ¥æ‰¾ç« èŠ‚ç›¸å…³é“¾æ¥...")
            chapter_links = []
            for link in all_links:
                text = link.get_text(strip=True)
                href = link.get("href", "")
                if any(keyword in text for keyword in ["ç¬¬", "ç« ", "èŠ‚", "å›"]) or "txt" in href:
                    chapter_links.append((text, href))
            
            print(f"   ç« èŠ‚ç›¸å…³é“¾æ¥æ•°: {len(chapter_links)}")
            for i, (text, href) in enumerate(chapter_links[:10]):
                print(f"     {i+1}. {text} - {href}")
            
            # 5. æµ‹è¯•é€‰æ‹©å™¨
            print("\n5. æµ‹è¯•é€‰æ‹©å™¨...")
            selectors = [
                ".catalog li a",
                ".catalog > div > ul > ul > li > a",
                ".catalog a",
                "li a",
                "a",
                "a[href*='txt']",
                "a[href*='.html']"
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                print(f"   é€‰æ‹©å™¨ '{selector}': æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                if elements:
                    print(f"     ç¬¬ä¸€ä¸ªå…ƒç´ : {elements[0].get_text(strip=True)}")
                    print(f"     ç¬¬ä¸€ä¸ªhref: {elements[0].get('href', 'æ— href')}")
            
            # 6. æŸ¥æ‰¾catalogå…ƒç´ 
            print("\n6. æŸ¥æ‰¾catalogå…ƒç´ ...")
            catalog_elements = soup.find_all(class_=lambda x: x and "catalog" in x)
            print(f"   catalogç›¸å…³å…ƒç´ : {len(catalog_elements)}")
            
            for i, elem in enumerate(catalog_elements):
                print(f"     {i+1}. {elem}")
                links = elem.find_all("a")
                print(f"       åŒ…å« {len(links)} ä¸ªé“¾æ¥")
                if links:
                    for j, link in enumerate(links[:3]):
                        print(f"         {j+1}. {link.get_text(strip=True)} - {link.get('href', 'æ— href')}")
            
            # 7. æµ‹è¯•API
            print("\n7. æµ‹è¯•API...")
            api_response = requests.get(
                "http://localhost:8000/api/novels/toc",
                params={
                    "url": test_url,
                    "sourceId": 11
                },
                timeout=30
            )
            
            print(f"   APIå“åº”çŠ¶æ€ç : {api_response.status_code}")
            if api_response.status_code == 200:
                data = api_response.json()
                chapters = data.get("data", [])
                print(f"   APIè¿”å›ç« èŠ‚æ•°: {len(chapters)}")
                
                if chapters:
                    print("   APIç« èŠ‚é¢„è§ˆ:")
                    for i, chapter in enumerate(chapters[:3]):
                        print(f"     {i+1}. {chapter.get('title', 'æ— æ ‡é¢˜')}")
                else:
                    print("   âŒ APIè¿”å›ç©ºç« èŠ‚åˆ—è¡¨")
            else:
                print(f"   âŒ APIè¯·æ±‚å¤±è´¥: {api_response.text}")
                
        else:
            print(f"   âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            
    except Exception as e:
        print(f"   âŒ è°ƒè¯•å¼‚å¸¸: {str(e)}")

def test_different_source():
    """æµ‹è¯•ä¸åŒçš„ä¹¦æº"""
    print("\nğŸ”§ æµ‹è¯•ä¸åŒçš„ä¹¦æº")
    print("=" * 50)
    
    # æµ‹è¯•ä¸åŒçš„ä¹¦æº
    test_sources = [
        {
            "id": 2,
            "name": "ä¹¦æµ·é˜å°è¯´ç½‘",
            "url": "https://www.shuhaige.net/"
        },
        {
            "id": 8,
            "name": "å¤§ç†ŠçŒ«æ–‡å­¦",
            "url": "https://www.dxmwx.org/"
        },
        {
            "id": 12,
            "name": "å¾—å¥‡å°è¯´ç½‘",
            "url": "https://www.deqixs.com/"
        }
    ]
    
    for source in test_sources:
        print(f"\næµ‹è¯•ä¹¦æº: {source['name']}")
        
        try:
            # æœç´¢è·å–URL
            search_response = requests.get(
                "http://localhost:8000/api/novels/search",
                params={"keyword": "æ–—ç ´è‹ç©¹", "maxResults": 1},
                timeout=30
            )
            
            if search_response.status_code == 200:
                search_data = search_response.json()
                results = search_data.get("data", [])
                if results:
                    result = results[0]
                    test_url = result.get("url")
                    
                    print(f"  æµ‹è¯•URL: {test_url}")
                    
                    # æµ‹è¯•ç›®å½•è§£æ
                    toc_response = requests.get(
                        "http://localhost:8000/api/novels/toc",
                        params={
                            "url": test_url,
                            "sourceId": source["id"]
                        },
                        timeout=30
                    )
                    
                    if toc_response.status_code == 200:
                        toc_data = toc_response.json()
                        chapters = toc_data.get("data", [])
                        print(f"  ç›®å½•ç« èŠ‚æ•°: {len(chapters)}")
                        
                        if chapters:
                            print(f"  âœ… ä¹¦æº {source['name']} å¯ç”¨")
                            return source
                        else:
                            print(f"  âŒ ä¹¦æº {source['name']} ç›®å½•ä¸ºç©º")
                    else:
                        print(f"  âŒ ä¹¦æº {source['name']} ç›®å½•APIå¤±è´¥")
                else:
                    print(f"  âŒ ä¹¦æº {source['name']} æœç´¢æ— ç»“æœ")
            else:
                print(f"  âŒ ä¹¦æº {source['name']} æœç´¢APIå¤±è´¥")
                
        except Exception as e:
            print(f"  âŒ ä¹¦æº {source['name']} æµ‹è¯•å¼‚å¸¸: {str(e)}")
    
    return None

if __name__ == "__main__":
    # è¿è¡Œç®€åŒ–è°ƒè¯•
    simple_toc_debug()
    test_different_source()