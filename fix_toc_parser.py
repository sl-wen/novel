#!/usr/bin/env python3
"""
ä¿®å¤ç›®å½•è§£æå™¨
"""

def fix_toc_parser():
    """ä¿®å¤ç›®å½•è§£æå™¨çš„é—®é¢˜"""
    print("ğŸ”§ ä¿®å¤ç›®å½•è§£æå™¨...")
    
    # 1. åˆ†æé—®é¢˜
    print("\n1. é—®é¢˜åˆ†æ:")
    print("   - æœ¬åœ°æ¨¡æ‹ŸæˆåŠŸï¼Œå¯ä»¥æ‰¾åˆ°15ä¸ªç« èŠ‚")
    print("   - APIè¿”å›ç©ºæ•°ç»„")
    print("   - é—®é¢˜åœ¨äºç›®å½•è§£æå™¨çš„ç½‘ç»œè¯·æ±‚æˆ–é”™è¯¯å¤„ç†")
    
    # 2. ä¿®å¤æ–¹æ¡ˆ
    print("\n2. ä¿®å¤æ–¹æ¡ˆ:")
    fix_content = '''
# ä¿®å¤ app/parsers/toc_parser.py ä¸­çš„é—®é¢˜

# 1. æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—
import logging
logger = logging.getLogger(__name__)

# 2. ä¿®å¤ _fetch_html æ–¹æ³•ï¼Œæ·»åŠ æ›´å¤šé”™è¯¯å¤„ç†
async def _fetch_html(self, url: str) -> Optional[str]:
    """è·å–HTMLé¡µé¢"""
    try:
        logger.info(f"å¼€å§‹è·å–HTML: {url}")
        
        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œè·³è¿‡è¯ä¹¦éªŒè¯
        connector = aiohttp.TCPConnector(
            limit=settings.MAX_CONCURRENT_REQUESTS,
            ssl=False,  # è·³è¿‡SSLè¯ä¹¦éªŒè¯
            use_dns_cache=True,
            ttl_dns_cache=300,
        )
        
        timeout = aiohttp.ClientTimeout(
            total=self.timeout,
            connect=10,
            sock_read=30
        )
        
        # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        async with aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers=headers
        ) as session:
            async with session.get(url) as response:
                logger.info(f"å“åº”çŠ¶æ€ç : {response.status}")
                if response.status == 200:
                    html = await response.text()
                    logger.info(f"è·å–HTMLæˆåŠŸï¼Œé•¿åº¦: {len(html)}")
                    return html
                else:
                    logger.error(f"è¯·æ±‚å¤±è´¥: {url}, çŠ¶æ€ç : {response.status}")
                    return None
    except Exception as e:
        logger.error(f"è¯·æ±‚å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
        return None

# 3. ä¿®å¤ _parse_toc æ–¹æ³•ï¼Œæ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
def _parse_toc(self, html: str, toc_url: str) -> List[ChapterInfo]:
    """è§£æç›®å½•"""
    logger.info(f"å¼€å§‹è§£æç›®å½•ï¼ŒHTMLé•¿åº¦: {len(html)}")
    
    soup = BeautifulSoup(html, "html.parser")
    chapters = []

    # è·å–ç« èŠ‚åˆ—è¡¨é€‰æ‹©å™¨
    list_selectors = self.toc_rule.get("list", "").split(",")
    if not list_selectors:
        logger.warning("æœªé…ç½®ç« èŠ‚åˆ—è¡¨é€‰æ‹©å™¨")
        return chapters

    # å°è¯•å¤šä¸ªé€‰æ‹©å™¨è·å–ç« èŠ‚åˆ—è¡¨
    chapter_elements = []
    for selector in list_selectors:
        selector = selector.strip()
        if not selector:
            continue
            
        elements = soup.select(selector)
        logger.info(f"é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
        if elements:
            chapter_elements = elements
            break
    
    if not chapter_elements:
        logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚å…ƒç´ ")
        return chapters

    logger.info(f"å¼€å§‹è§£æ {len(chapter_elements)} ä¸ªç« èŠ‚å…ƒç´ ")
    
    for index, element in enumerate(chapter_elements, 1):
        try:
            chapter = self._parse_single_chapter(element, toc_url, index)
            if chapter:
                chapters.append(chapter)
                logger.debug(f"æˆåŠŸè§£æç« èŠ‚ {index}: {chapter.title}")
            else:
                logger.warning(f"è§£æç« èŠ‚ {index} å¤±è´¥")
        except Exception as e:
            logger.warning(f"è§£æå•ä¸ªç« èŠ‚å¤±è´¥: {str(e)}")
            continue

    logger.info(f"ç›®å½•è§£æå®Œæˆï¼ŒæˆåŠŸè§£æ {len(chapters)} ä¸ªç« èŠ‚")
    return chapters

# 4. ä¿®å¤ _parse_single_chapter æ–¹æ³•
def _parse_single_chapter(
    self, element: BeautifulSoup, toc_url: str, order: int
) -> Optional[ChapterInfo]:
    """è§£æå•ä¸ªç« èŠ‚"""
    try:
        # è·å–ç« èŠ‚æ ‡é¢˜
        title_selector = self.toc_rule.get("title", "")
        if title_selector == "text":
            # ç›´æ¥ä½¿ç”¨å…ƒç´ æ–‡æœ¬
            title = element.get_text(strip=True)
        else:
            title = self._extract_text(element, title_selector)

        # è·å–ç« èŠ‚URL
        url_selector = self.toc_rule.get("url", "")
        if url_selector == "href":
            # ç›´æ¥ä½¿ç”¨hrefå±æ€§
            url = element.get("href", "")
        else:
            url = self._extract_attr(element, url_selector, "href")

        # æ„å»ºå®Œæ•´URL
        if url and not url.startswith(("http://", "https://")):
            base_url = self.source.rule.get("url", "")
            url = f"{base_url.rstrip('/')}/{url.lstrip('/')}"

        # è·å–ç« èŠ‚å­—æ•°ï¼ˆå¯é€‰ï¼‰
        word_count_selector = self.toc_rule.get("word_count", "")
        word_count = self._extract_text(element, word_count_selector)

        # è·å–æ›´æ–°æ—¶é—´ï¼ˆå¯é€‰ï¼‰
        update_time_selector = self.toc_rule.get("update_time", "")
        update_time = self._extract_text(element, update_time_selector)

        # éªŒè¯å¿…è¦å­—æ®µ
        if not title or not url:
            logger.warning(f"ç« èŠ‚ {order} ç¼ºå°‘å¿…è¦å­—æ®µ: title='{title}', url='{url}'")
            return None

        return ChapterInfo(
            title=title or f"ç¬¬{order}ç« ",
            url=url or "",
            order=order,
            word_count=word_count or "",
            update_time=update_time or "",
            source_id=self.source.id,
            source_name=self.source.rule.get("name", self.source.id),
        )
    except Exception as e:
        logger.warning(f"è§£æç« èŠ‚å¤±è´¥: {str(e)}")
        return None
'''
    print(fix_content)
    
    # 3. åˆ›å»ºæµ‹è¯•è„šæœ¬
    print("\n3. åˆ›å»ºæµ‹è¯•è„šæœ¬:")
    test_script = '''
#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„ç›®å½•è§£æå™¨
"""

import requests
import json

def test_fixed_toc_parser():
    """æµ‹è¯•ä¿®å¤åçš„ç›®å½•è§£æå™¨"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„ç›®å½•è§£æå™¨...")
    
    base_url = "http://localhost:8000"
    
    # æµ‹è¯•è·å–ç›®å½•
    try:
        response = requests.get(
            f"{base_url}/api/novels/toc",
            params={
                "url": "https://www.0xs.net/txt/1.html",
                "sourceId": 11
            },
            timeout=60
        )
        
        print(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            chapters = data.get("data", [])
            print(f"âœ… è·å–ç›®å½•æˆåŠŸ: {len(chapters)} ç« ")
            
            if chapters:
                print("å‰5ç« :")
                for i, chapter in enumerate(chapters[:5]):
                    print(f"  {i+1}. {chapter.get('title', 'Unknown')}")
                    print(f"     URL: {chapter.get('url', 'Unknown')}")
                
                # æµ‹è¯•ä¸‹è½½
                print("\\næµ‹è¯•ä¸‹è½½...")
                download_response = requests.get(
                    f"{base_url}/api/novels/download",
                    params={
                        "url": "https://www.0xs.net/txt/1.html",
                        "sourceId": 11,
                        "format": "txt"
                    },
                    timeout=300,
                    stream=True
                )
                
                print(f"ä¸‹è½½çŠ¶æ€ç : {download_response.status_code}")
                if download_response.status_code == 200:
                    # ä¿å­˜æµ‹è¯•æ–‡ä»¶
                    filename = "test_fixed_download.txt"
                    with open(filename, "wb") as f:
                        for chunk in download_response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                    
                    import os
                    file_size = os.path.getsize(filename)
                    print(f"âœ… ä¸‹è½½æˆåŠŸï¼Œæ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
                    
                    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
                    os.remove(filename)
                    print("æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")
                    return True
                else:
                    print(f"âŒ ä¸‹è½½å¤±è´¥: {download_response.text}")
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç« èŠ‚")
                
        else:
            print(f"âŒ è·å–ç›®å½•å¤±è´¥: {response.text}")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    test_fixed_toc_parser()
'''
    print(test_script)

if __name__ == "__main__":
    fix_toc_parser()