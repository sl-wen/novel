#!/usr/bin/env python3
"""
ä¿®å¤ç›®å½•è§£æå™¨
"""

def fix_toc_parser():
    """ä¿®å¤ç›®å½•è§£æå™¨çš„é—®é¢˜"""
    print("ğŸ”§ ä¿®å¤ç›®å½•è§£æå™¨...")
    
    # 1. åˆ†æé—®é¢˜
    print("\n1. é—®é¢˜åˆ†æ:")
    print("   - ç›®å½•è§„åˆ™ä¸­çš„titleå’Œurlé€‰æ‹©å™¨ä¸ºç©º")
    print("   - æ— æ³•æå–ç« èŠ‚æ ‡é¢˜å’ŒURL")
    print("   - éœ€è¦ä¿®å¤ä¹¦æºè§„åˆ™é…ç½®")
    
    # 2. ä¿®å¤æ–¹æ¡ˆ
    print("\n2. ä¿®å¤æ–¹æ¡ˆ:")
    fix_content = '''
# ä¿®å¤ rules/rule-11.json ä¸­çš„ç›®å½•é…ç½®

{
  "toc": {
    "list": ".catalog > div > ul > ul > li > a",
    "title": "text",  # ä½¿ç”¨å…ƒç´ æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
    "url": "href",    # ä½¿ç”¨hrefå±æ€§ä½œä¸ºURL
    "has_pages": false,
    "next_page": ""
  }
}

# ä¿®æ”¹ app/parsers/toc_parser.py ä¸­çš„ _parse_single_chapter æ–¹æ³•

def _parse_single_chapter(
    self, element: BeautifulSoup, toc_url: str, order: int
) -> Optional[ChapterInfo]:
    """è§£æå•ä¸ªç« èŠ‚"""
    try:
        # è·å–ç« èŠ‚æ ‡é¢˜
        title_selector = self.toc_rule.get("title", "")
        if title_selector == "text":
            # ç›´æ¥ä½¿ç”¨å…ƒç´ æ–‡æœ¬
            title = element.get_text(strip=True)
        else:
            title = self._extract_text(element, title_selector)

        # è·å–ç« èŠ‚URL
        url_selector = self.toc_rule.get("url", "")
        if url_selector == "href":
            # ç›´æ¥ä½¿ç”¨hrefå±æ€§
            url = element.get("href", "")
        else:
            url = self._extract_attr(element, url_selector, "href")

        # æ„å»ºå®Œæ•´URL
        if url and not url.startswith(("http://", "https://")):
            base_url = self.source.rule.get("url", "")
            url = f"{base_url.rstrip('/')}/{url.lstrip('/')}"

        # è·å–ç« èŠ‚å­—æ•°ï¼ˆå¯é€‰ï¼‰
        word_count_selector = self.toc_rule.get("word_count", "")
        word_count = self._extract_text(element, word_count_selector)

        # è·å–æ›´æ–°æ—¶é—´ï¼ˆå¯é€‰ï¼‰
        update_time_selector = self.toc_rule.get("update_time", "")
        update_time = self._extract_text(element, update_time_selector)

        return ChapterInfo(
            title=title or f"ç¬¬{order}ç« ",
            url=url or "",
            order=order,
            word_count=word_count or "",
            update_time=update_time or "",
            source_id=self.source.id,
            source_name=self.source.rule.get("name", self.source.id),
        )
    except Exception as e:
        logger.warning(f"è§£æç« èŠ‚å¤±è´¥: {str(e)}")
        return None
'''
    print(fix_content)
    
    # 3. åˆ›å»ºæµ‹è¯•è„šæœ¬
    print("\n3. åˆ›å»ºæµ‹è¯•è„šæœ¬:")
    test_script = '''
#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„ç›®å½•è§£æå™¨
"""

import requests
import json

def test_fixed_toc_parser():
    """æµ‹è¯•ä¿®å¤åçš„ç›®å½•è§£æå™¨"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„ç›®å½•è§£æå™¨...")
    
    base_url = "http://localhost:8000"
    
    # æµ‹è¯•è·å–ç›®å½•
    try:
        response = requests.get(
            f"{base_url}/api/novels/toc",
            params={
                "url": "https://www.0xs.net/txt/1.html",
                "sourceId": 11
            },
            timeout=60
        )
        
        print(f"çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 200:
            data = response.json()
            chapters = data.get("data", [])
            print(f"âœ… è·å–ç›®å½•æˆåŠŸ: {len(chapters)} ç« ")
            
            if chapters:
                print("å‰5ç« :")
                for i, chapter in enumerate(chapters[:5]):
                    print(f"  {i+1}. {chapter.get('title', 'Unknown')}")
                    print(f"     URL: {chapter.get('url', 'Unknown')}")
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç« èŠ‚")
                
        else:
            print(f"âŒ è·å–ç›®å½•å¤±è´¥: {response.text}")
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")

if __name__ == "__main__":
    test_fixed_toc_parser()
'''
    print(test_script)

if __name__ == "__main__":
    fix_toc_parser()