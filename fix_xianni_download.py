#!/usr/bin/env python3
"""
ã€Šä»™é€†ã€‹ä¸‹è½½é—®é¢˜ä¿®å¤è„šæœ¬
ä¸“é—¨è§£å†³ç« èŠ‚è·å–å¤±è´¥å’Œå†…å®¹ä¸æ­£ç¡®çš„é—®é¢˜
"""

import asyncio
import json
import logging
import os
import re
import sys
import time
from pathlib import Path
from typing import List, Optional

import aiohttp
from bs4 import BeautifulSoup

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from app.core.config import settings
from app.core.source import Source
from app.models.book import Book
from app.models.chapter import Chapter, ChapterInfo
from app.parsers.book_parser import BookParser
from app.parsers.chapter_parser import ChapterParser
from app.parsers.toc_parser import TocParser

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class XianNiDownloader:
    """ã€Šä»™é€†ã€‹ä¸“ç”¨ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.session = None
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/91.0.4472.124 Safari/537.36"
            ),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(
            limit=10,
            limit_per_host=5,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=self.headers
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def fetch_html(self, url: str, retries: int = 3) -> Optional[str]:
        """è·å–HTMLå†…å®¹"""
        for attempt in range(retries):
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        content = await response.text()
                        return content
                    else:
                        logger.warning(f"HTTP {response.status}: {url}")
                        
            except Exception as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {str(e)}")
                if attempt < retries - 1:
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    
        return None
    
    async def get_book_info(self, url: str) -> Optional[Book]:
        """è·å–ä¹¦ç±ä¿¡æ¯"""
        html = await self.fetch_html(url)
        if not html:
            return None
            
        soup = BeautifulSoup(html, "html.parser")
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–ä¹¦å
        title_selectors = [
            "meta[property='og:novel:book_name']",
            "meta[name='og:novel:book_name']",
            "h1",
            ".bookname h1",
            "#info h1",
            ".book-title",
        ]
        
        title = None
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True) or element.get("content", "")
                if title:
                    break
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–ä½œè€…
        author_selectors = [
            "meta[property='og:novel:author']",
            "meta[name='og:novel:author']",
            "#info p:first-child",
            ".book-author",
            "p:contains('ä½œè€…')",
        ]
        
        author = None
        for selector in author_selectors:
            element = soup.select_one(selector)
            if element:
                author = element.get_text(strip=True) or element.get("content", "")
                if author:
                    break
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨è·å–ç®€ä»‹
        intro_selectors = [
            "meta[property='og:description']",
            "meta[name='og:description']",
            "#intro",
            ".book-intro",
            ".intro",
        ]
        
        intro = None
        for selector in intro_selectors:
            element = soup.select_one(selector)
            if element:
                intro = element.get_text(strip=True) or element.get("content", "")
                if intro:
                    break
        
        return Book(
            title=title or "ä»™é€†",
            author=author or "è€³æ ¹",
            intro=intro or "",
            url=url
        )
    
    async def get_toc(self, url: str) -> List[ChapterInfo]:
        """è·å–ç›®å½•"""
        html = await self.fetch_html(url)
        if not html:
            return []
            
        soup = BeautifulSoup(html, "html.parser")
        
        # å°è¯•å¤šç§ç›®å½•é€‰æ‹©å™¨
        toc_selectors = [
            "#list dd a",
            ".listmain dd a",
            "#chapter-list a",
            ".chapter-list a",
            "dl dd a",
            ".list a",
        ]
        
        chapters = []
        for selector in toc_selectors:
            links = soup.select(selector)
            if links:
                for i, link in enumerate(links, 1):
                    title = link.get_text(strip=True)
                    href = link.get("href", "")
                    
                    if title and href:
                        # å¤„ç†ç›¸å¯¹URL
                        if href.startswith("/"):
                            base_url = "/".join(url.split("/")[:3])
                            full_url = base_url + href
                        elif href.startswith("http"):
                            full_url = href
                        else:
                            full_url = url.rstrip("/") + "/" + href.lstrip("/")
                        
                        chapters.append(ChapterInfo(
                            title=title,
                            url=full_url,
                            order=i
                        ))
                
                if chapters:
                    logger.info(f"æ‰¾åˆ° {len(chapters)} ç« ")
                    break
        
        return chapters
    
    async def get_chapter_content(self, url: str, title: str) -> Optional[Chapter]:
        """è·å–ç« èŠ‚å†…å®¹"""
        html = await self.fetch_html(url)
        if not html:
            return None
            
        soup = BeautifulSoup(html, "html.parser")
        
        # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            "#content",
            ".content",
            ".chapter-content",
            "#chapter-content",
            ".text",
            "#text",
        ]
        
        content = None
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(separator="\n", strip=True)
                if content and len(content) > 100:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                    break
        
        if not content:
            return None
        
        # æ¸…ç†å†…å®¹
        content = self._clean_content(content)
        
        return Chapter(
            title=title,
            content=content,
            url=url,
            order=0
        )
    
    def _clean_content(self, content: str) -> str:
        """æ¸…ç†ç« èŠ‚å†…å®¹"""
        if not content:
            return ""
        
        # ç§»é™¤å¹¿å‘Šå’Œåƒåœ¾å†…å®¹
        ad_patterns = [
            r"ä¸€ç§’è®°ä½ã€.+?ã€‘",
            r"å¤©æ‰ä¸€ç§’è®°ä½.+?",
            r"å¤©æ‰å£¹ç§’è¨˜ä½.+?",
            r"çœ‹æœ€æ–°ç« èŠ‚è¯·åˆ°.+?",
            r"æœ¬ä¹¦æœ€æ–°ç« èŠ‚è¯·åˆ°.+?",
            r"æ›´æ–°æœ€å¿«çš„.+?",
            r"æ‰‹æœºç”¨æˆ·è¯·è®¿é—®.+?",
            r"æ‰‹æœºç‰ˆé˜…è¯»ç½‘å€.+?",
            r"æ¨èéƒ½å¸‚å¤§ç¥.+?",
            r"\(æœ¬ç« å®Œ\)",
            r"ç« èŠ‚é”™è¯¯.+?ä¸¾æŠ¥",
            r"å†…å®¹ä¸¥é‡ç¼ºå¤±.+?ä¸¾æŠ¥",
            r"ç¬”è¶£é˜.+?",
            r"æ–°ç¬”è¶£é˜.+?",
            r"é¦™ä¹¦å°è¯´.+?",
        ]
        
        for pattern in ad_patterns:
            content = re.sub(pattern, "", content, flags=re.IGNORECASE)
        
        # ç§»é™¤å¤šä½™ç©ºè¡Œ
        content = re.sub(r"\n\s*\n\s*\n+", "\n\n", content)
        
        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
        lines = [line.strip() for line in content.split("\n") if line.strip()]
        
        return "\n".join(lines)
    
    async def download_xianni(self, url: str, output_file: str = None) -> str:
        """ä¸‹è½½ã€Šä»™é€†ã€‹"""
        print(f"å¼€å§‹ä¸‹è½½ã€Šä»™é€†ã€‹: {url}")
        
        # è·å–ä¹¦ç±ä¿¡æ¯
        book = await self.get_book_info(url)
        if not book:
            raise Exception("æ— æ³•è·å–ä¹¦ç±ä¿¡æ¯")
        
        print(f"âœ… ä¹¦ç±ä¿¡æ¯: {book.title} - {book.author}")
        
        # è·å–ç›®å½•
        toc = await self.get_toc(url)
        if not toc:
            raise Exception("æ— æ³•è·å–ç›®å½•")
        
        print(f"âœ… ç›®å½•è·å–: {len(toc)} ç« ")
        
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶
        if not output_file:
            output_file = f"ä»™é€†_{book.author}.txt"
        
        # ä¸‹è½½ç« èŠ‚å†…å®¹
        print("å¼€å§‹ä¸‹è½½ç« èŠ‚å†…å®¹...")
        chapters = []
        failed_chapters = []
        
        for i, chapter_info in enumerate(toc, 1):
            print(f"ä¸‹è½½ç¬¬ {i}/{len(toc)} ç« : {chapter_info.title}")
            
            try:
                chapter = await self.get_chapter_content(chapter_info.url, chapter_info.title)
                if chapter and len(chapter.content) > 50:
                    chapters.append(chapter)
                    print(f"  âœ… æˆåŠŸ ({len(chapter.content)} å­—ç¬¦)")
                else:
                    failed_chapters.append(chapter_info.title)
                    print(f"  âŒ å¤±è´¥ (å†…å®¹è¿‡çŸ­æˆ–æ— å†…å®¹)")
            except Exception as e:
                failed_chapters.append(chapter_info.title)
                print(f"  âŒ å¤±è´¥: {str(e)}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
            await asyncio.sleep(1)
        
        print(f"\nä¸‹è½½å®Œæˆ: æˆåŠŸ {len(chapters)} ç« ï¼Œå¤±è´¥ {len(failed_chapters)} ç« ")
        
        if failed_chapters:
            print("å¤±è´¥çš„ç« èŠ‚:")
            for title in failed_chapters[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {title}")
        
        # ç”ŸæˆTXTæ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"ä¹¦å: {book.title}\n")
            f.write(f"ä½œè€…: {book.author}\n")
            f.write(f"ç®€ä»‹: {book.intro}\n")
            f.write(f"åˆ†ç±»: ä»™ä¾ å°è¯´\n")
            f.write(f"æœ€æ–°ç« èŠ‚: \n")
            f.write(f"æ›´æ–°æ—¶é—´: \n")
            f.write(f"çŠ¶æ€: å·²å®Œç»“\n")
            f.write(f"å­—æ•°: \n")
            f.write("\n" + "=" * 50 + "\n\n")
            
            for chapter in chapters:
                f.write(f"\n\n{chapter.title}\n\n")
                f.write(chapter.content)
        
        print(f"âœ… æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {output_file}")
        print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(output_file)} å­—èŠ‚")
        
        return output_file


async def main():
    """ä¸»å‡½æ•°"""
    print("ã€Šä»™é€†ã€‹ä¸“ç”¨ä¸‹è½½å™¨")
    print("=" * 50)
    
    # é¢„è®¾çš„ã€Šä»™é€†ã€‹URLåˆ—è¡¨
    xianni_urls = [
        "http://www.xbiqugu.la/0_1/",
        "https://www.xbiquge.la/0_1/",
        "https://www.biquge.com.cn/book/1/",
        "https://www.biquge.tv/0_1/",
    ]
    
    print("å¯ç”¨çš„ã€Šä»™é€†ã€‹URL:")
    for i, url in enumerate(xianni_urls, 1):
        print(f"{i}. {url}")
    
    print("\nè¯·é€‰æ‹©URL (è¾“å…¥æ•°å­—) æˆ–ç›´æ¥è¾“å…¥URL:")
    choice = input("é€‰æ‹©: ").strip()
    
    if choice.isdigit():
        idx = int(choice) - 1
        if 0 <= idx < len(xianni_urls):
            url = xianni_urls[idx]
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
    else:
        url = choice
    
    async with XianNiDownloader() as downloader:
        try:
            output_file = await downloader.download_xianni(url)
            print(f"\nğŸ‰ ä¸‹è½½å®Œæˆ: {output_file}")
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    asyncio.run(main())