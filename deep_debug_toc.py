#!/usr/bin/env python3
"""
æ·±åº¦è°ƒè¯•ç›®å½•è§£æé—®é¢˜
è¯¦ç»†åˆ†æç›®å½•è§£æå¤±è´¥çš„åŸå› 
"""

import asyncio
import aiohttp
import logging
import requests
from bs4 import BeautifulSoup
from app.core.config import settings
from app.core.source import Source
from app.parsers.toc_parser import TocParser

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def deep_debug_toc_parser():
    """æ·±åº¦è°ƒè¯•ç›®å½•è§£æå™¨"""
    print("ğŸ” æ·±åº¦è°ƒè¯•ç›®å½•è§£æé—®é¢˜")
    print("=" * 60)
    
    # æµ‹è¯•URLå’Œä¹¦æº
    test_url = "https://www.0xs.net/txt/1.html"
    source_id = 11  # é›¶ç‚¹å°è¯´
    
    # åˆ›å»ºä¹¦æºå¯¹è±¡
    source_data = {
        "id": source_id,
        "name": "é›¶ç‚¹å°è¯´",
        "url": "https://www.0xs.net/",
        "enabled": True,
        "type": "html",
        "language": "zh_CN",
        "toc": {
            "list": ".catalog li a",
            "title": "text",
            "url": "href",
            "has_pages": False,
            "next_page": "",
            "baseUri": "https://www.0xs.net/",
            "timeout": 15
        }
    }
    
    source = Source(**source_data)
    parser = TocParser(source)
    
    print(f"æµ‹è¯•URL: {test_url}")
    print(f"ä¹¦æº: {source.name}")
    print(f"ç›®å½•è§„åˆ™: {source.rule.get('toc', {})}")
    
    # 1. æµ‹è¯•è·å–ç›®å½•URL
    print("\n1. æµ‹è¯•è·å–ç›®å½•URL...")
    toc_url = parser._get_toc_url(test_url)
    print(f"   ç›®å½•URL: {toc_url}")
    
    # 2. æµ‹è¯•è·å–HTML - è¯¦ç»†è°ƒè¯•
    print("\n2. æµ‹è¯•è·å–HTML - è¯¦ç»†è°ƒè¯•...")
    
    # åˆ›å»ºaiohttpä¼šè¯è¿›è¡Œè¯¦ç»†è°ƒè¯•
    connector = aiohttp.TCPConnector(
        limit=5,
        ssl=False,
        use_dns_cache=True,
        ttl_dns_cache=300,
    )
    
    timeout = aiohttp.ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(
        connector=connector,
        timeout=timeout,
        headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://www.0xs.net/",
        }
    ) as session:
        try:
            print(f"   æ­£åœ¨è¯·æ±‚: {toc_url}")
            async with session.get(toc_url) as response:
                print(f"   å“åº”çŠ¶æ€ç : {response.status}")
                print(f"   å“åº”å¤´: {dict(response.headers)}")
                
                if response.status == 200:
                    html = await response.text()
                    print(f"   HTMLé•¿åº¦: {len(html)}")
                    print(f"   HTMLé¢„è§ˆ: {html[:500]}...")
                    
                    # 3. è¯¦ç»†åˆ†æHTMLç»“æ„
                    print("\n3. è¯¦ç»†åˆ†æHTMLç»“æ„...")
                    soup = BeautifulSoup(html, "html.parser")
                    
                    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ç›®å½•å…ƒç´ 
                    print("   æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ç›®å½•å…ƒç´ ...")
                    
                    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    all_links = soup.find_all("a")
                    print(f"   æ€»é“¾æ¥æ•°: {len(all_links)}")
                    
                    # æŸ¥æ‰¾åŒ…å«ç« èŠ‚ç›¸å…³æ–‡æœ¬çš„é“¾æ¥
                    chapter_links = []
                    for link in all_links:
                        text = link.get_text(strip=True)
                        href = link.get("href", "")
                        if any(keyword in text for keyword in ["ç¬¬", "ç« ", "èŠ‚", "å›", "txt"]):
                            chapter_links.append((text, href))
                    
                    print(f"   å¯èƒ½çš„ç« èŠ‚é“¾æ¥: {len(chapter_links)}")
                    for i, (text, href) in enumerate(chapter_links[:10]):
                        print(f"     {i+1}. {text} - {href}")
                    
                    # æŸ¥æ‰¾catalogç›¸å…³å…ƒç´ 
                    catalog_elements = soup.find_all(class_=lambda x: x and "catalog" in x)
                    print(f"   catalogç›¸å…³å…ƒç´ : {len(catalog_elements)}")
                    
                    # æŸ¥æ‰¾listç›¸å…³å…ƒç´ 
                    list_elements = soup.find_all(class_=lambda x: x and "list" in x)
                    print(f"   listç›¸å…³å…ƒç´ : {len(list_elements)}")
                    
                    # 4. æµ‹è¯•ä¸åŒçš„é€‰æ‹©å™¨
                    print("\n4. æµ‹è¯•ä¸åŒçš„é€‰æ‹©å™¨...")
                    selectors = [
                        ".catalog li a",
                        ".catalog > div > ul > ul > li > a",
                        ".catalog a",
                        "li a",
                        "a",
                        "a[href*='txt']",
                        "a[href*='.html']",
                        ".chapter-list a",
                        ".chapter a",
                        "ul li a",
                        "div ul li a"
                    ]
                    
                    for selector in selectors:
                        elements = soup.select(selector)
                        print(f"   é€‰æ‹©å™¨ '{selector}': æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        if elements:
                            print(f"     ç¬¬ä¸€ä¸ªå…ƒç´ : {elements[0]}")
                            print(f"     ç¬¬ä¸€ä¸ªå…ƒç´ æ–‡æœ¬: {elements[0].get_text(strip=True)}")
                            print(f"     ç¬¬ä¸€ä¸ªå…ƒç´ href: {elements[0].get('href', 'æ— href')}")
                    
                    # 5. æµ‹è¯•å®Œæ•´è§£æ
                    print("\n5. æµ‹è¯•å®Œæ•´è§£æ...")
                    chapters = await parser.parse(test_url)
                    print(f"   è§£æç»“æœ: {len(chapters)} ä¸ªç« èŠ‚")
                    
                    if chapters:
                        print("   ç« èŠ‚é¢„è§ˆ:")
                        for i, chapter in enumerate(chapters[:5]):
                            print(f"     {i+1}. {chapter.title} - {chapter.url}")
                    else:
                        print("   âŒ æ²¡æœ‰è§£æåˆ°ç« èŠ‚")
                        
                        # 6. è¯¦ç»†è°ƒè¯•è§£æè¿‡ç¨‹
                        print("\n6. è¯¦ç»†è°ƒè¯•è§£æè¿‡ç¨‹...")
                        chapters = parser._parse_toc(html, toc_url)
                        print(f"   ç›´æ¥è§£æç»“æœ: {len(chapters)} ä¸ªç« èŠ‚")
                        
                        # 7. åˆ†æè§£æå¤±è´¥çš„åŸå› 
                        print("\n7. åˆ†æè§£æå¤±è´¥çš„åŸå› ...")
                        
                        # æ£€æŸ¥é€‰æ‹©å™¨æ˜¯å¦æ­£ç¡®
                        list_selector = parser.toc_rule.get("list", "")
                        print(f"   é…ç½®çš„é€‰æ‹©å™¨: {list_selector}")
                        
                        elements = soup.select(list_selector)
                        print(f"   é€‰æ‹©å™¨æ‰¾åˆ°çš„å…ƒç´ æ•°: {len(elements)}")
                        
                        if elements:
                            print("   å‰5ä¸ªå…ƒç´ :")
                            for i, elem in enumerate(elements[:5]):
                                print(f"     {i+1}. {elem}")
                                print(f"        æ–‡æœ¬: {elem.get_text(strip=True)}")
                                print(f"        href: {elem.get('href', 'æ— href')}")
                        else:
                            print("   âŒ é€‰æ‹©å™¨æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…ƒç´ ")
                            
                            # å°è¯•æŸ¥æ‰¾å¯èƒ½çš„çˆ¶å…ƒç´ 
                            print("   å°è¯•æŸ¥æ‰¾å¯èƒ½çš„çˆ¶å…ƒç´ ...")
                            parent_selectors = [
                                ".catalog",
                                ".chapter-list",
                                ".chapter",
                                ".toc",
                                ".directory"
                            ]
                            
                            for parent_selector in parent_selectors:
                                parent_elements = soup.select(parent_selector)
                                if parent_elements:
                                    print(f"     æ‰¾åˆ°çˆ¶å…ƒç´  '{parent_selector}': {len(parent_elements)} ä¸ª")
                                    for parent in parent_elements:
                                        links = parent.find_all("a")
                                        print(f"       åŒ…å« {len(links)} ä¸ªé“¾æ¥")
                                        if links:
                                            print(f"        ç¬¬ä¸€ä¸ªé“¾æ¥: {links[0].get_text(strip=True)}")
                                            print(f"        ç¬¬ä¸€ä¸ªhref: {links[0].get('href', 'æ— href')}")
                
                else:
                    print(f"   âŒ è¯·æ±‚å¤±è´¥: {response.status}")
                    
        except Exception as e:
            print(f"   âŒ è¯·æ±‚å¼‚å¸¸: {str(e)}")

async def test_api_toc_endpoint():
    """æµ‹è¯•APIç›®å½•ç«¯ç‚¹"""
    print("\nğŸ”§ æµ‹è¯•APIç›®å½•ç«¯ç‚¹")
    print("=" * 60)
    
    base_url = "http://localhost:8000"
    
    try:
        # æµ‹è¯•ç›®å½•API
        response = requests.get(
            f"{base_url}/api/novels/toc",
            params={
                "url": "https://www.0xs.net/txt/1.html",
                "sourceId": 11
            },
            timeout=30
        )
        
        print(f"APIå“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"APIå“åº”å†…å®¹: {response.text}")
        
        if response.status_code == 200:
            data = response.json()
            chapters = data.get("data", [])
            print(f"APIè¿”å›ç« èŠ‚æ•°: {len(chapters)}")
            
            if chapters:
                print("APIç« èŠ‚é¢„è§ˆ:")
                for i, chapter in enumerate(chapters[:3]):
                    print(f"  {i+1}. {chapter.get('title', 'æ— æ ‡é¢˜')}")
            else:
                print("âŒ APIè¿”å›ç©ºç« èŠ‚åˆ—è¡¨")
        else:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.text}")
            
    except Exception as e:
        print(f"âŒ APIæµ‹è¯•å¼‚å¸¸: {str(e)}")

async def test_alternative_source():
    """æµ‹è¯•å…¶ä»–ä¹¦æº"""
    print("\nğŸ”§ æµ‹è¯•å…¶ä»–ä¹¦æº")
    print("=" * 60)
    
    # æµ‹è¯•ä¸åŒçš„ä¹¦æº
    test_sources = [
        {
            "id": 2,
            "name": "ä¹¦æµ·é˜å°è¯´ç½‘",
            "url": "https://www.shuhaige.net/",
            "toc": {
                "list": "#list > dl > dd > a",
                "title": "",
                "url": "",
                "has_pages": False,
                "next_page": "",
                "baseUri": "https://www.shuhaige.net/",
                "timeout": 15
            }
        },
        {
            "id": 8,
            "name": "å¤§ç†ŠçŒ«æ–‡å­¦",
            "url": "https://www.dxmwx.org/",
            "toc": {
                "list": "div:nth-child(2n+5) > span > a",
                "title": "",
                "url": "",
                "has_pages": False,
                "next_page": "",
                "baseUri": "https://www.dxmwx.org/",
                "timeout": 15
            }
        }
    ]
    
    for source_data in test_sources:
        print(f"\næµ‹è¯•ä¹¦æº: {source_data['name']}")
        
        source = Source(**source_data)
        parser = TocParser(source)
        
        # æµ‹è¯•æœç´¢è·å–URL
        try:
            search_response = requests.get(
                "http://localhost:8000/api/novels/search",
                params={"keyword": "æ–—ç ´è‹ç©¹", "maxResults": 1},
                timeout=30
            )
            
            if search_response.status_code == 200:
                search_data = search_response.json()
                results = search_data.get("data", [])
                if results:
                    result = results[0]
                    test_url = result.get("url")
                    
                    print(f"  æµ‹è¯•URL: {test_url}")
                    
                    # æµ‹è¯•ç›®å½•è§£æ
                    toc_response = requests.get(
                        "http://localhost:8000/api/novels/toc",
                        params={
                            "url": test_url,
                            "sourceId": source_data["id"]
                        },
                        timeout=30
                    )
                    
                    if toc_response.status_code == 200:
                        toc_data = toc_response.json()
                        chapters = toc_data.get("data", [])
                        print(f"  ç›®å½•ç« èŠ‚æ•°: {len(chapters)}")
                        
                        if chapters:
                            print(f"  âœ… ä¹¦æº {source_data['name']} å¯ç”¨")
                            return source_data
                        else:
                            print(f"  âŒ ä¹¦æº {source_data['name']} ç›®å½•ä¸ºç©º")
                    else:
                        print(f"  âŒ ä¹¦æº {source_data['name']} ç›®å½•APIå¤±è´¥")
                else:
                    print(f"  âŒ ä¹¦æº {source_data['name']} æœç´¢æ— ç»“æœ")
            else:
                print(f"  âŒ ä¹¦æº {source_data['name']} æœç´¢APIå¤±è´¥")
                
        except Exception as e:
            print(f"  âŒ ä¹¦æº {source_data['name']} æµ‹è¯•å¼‚å¸¸: {str(e)}")
    
    return None

if __name__ == "__main__":
    # è¿è¡Œæ·±åº¦è°ƒè¯•
    asyncio.run(deep_debug_toc_parser())
    asyncio.run(test_api_toc_endpoint())
    asyncio.run(test_alternative_source())